# ðŸ“‰ Customer Churn Prediction (Simulated Data)

This project explores how to build and evaluate machine learning models for predicting customer churn, using synthetic datasets tailored to reflect various real-world churn scenarios. It is part of a strategic portfolio demonstrating applied data science skills for business impact.

## ðŸ§  Goal

To simulate and analyze churn prediction models under different class imbalance settings:

- ðŸŸ¡ **Moderately imbalanced churn** (~20%)
- ðŸŸ  **Highly imbalanced churn** (<10%)
- ðŸ”´ **Rare event churn** (<5%)

## ðŸ§ª Whatâ€™s Included

- Simulated customer activity data
- ML models tailored to each churn scenario
- Model evaluation using:
  - ROC and PR curves
  - Confusion matrices
  - Threshold tuning for rare events

## âœ… Techniques Used

- Logistic Regression with class weights
- SMOTE oversampling for minority class
- XGBoost with scale adjustment
- Evaluation via `scikit-learn` metrics and visualizations

## ðŸ› ï¸ Tech Stack

- **Python 3.x**
- `pandas`, `numpy`, `matplotlib`, `seaborn`
- `scikit-learn`
- `xgboost`
- `imblearn` (for SMOTE)

## ðŸ“¦ How to Run

Install dependencies:

```bash
pip install -r requirements.txt
```


ðŸ“Œ This is a work in progress. Folder structure and final organization will evolve as the project grows.

#### Choice Of Base Model & Evaluation Metrics:

#### âœ… Churn Model Metrics
Choose metrics based on current priority

| Metric                                    | Why It's Useful                                                     | When to Use                                                                         |
| ----------------------------------------- | ------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **Recall (Sensitivity)**                  | Measures how many actual churners you caught                        | Most important when false negatives are costly (missing a churner = lost customer)  |
| **Precision**                             | Measures how many of the predicted churners really churned          | Important if actions are expensive (e.g., offering discounts only to real churners, not annoying loyal customers with unnecessary retention actions) |
| **F1 Score**                              | Harmonic mean of precision and recall                               | Useful when you want a balance between precision and recall                         |
| **ROC AUC Score**                         | Measures how well the model ranks positives over negatives          | Great for overall model evaluation and comparison                                   |
| **PR AUC Score** *(Precision-Recall AUC)* | Better than ROC AUC in highly imbalanced settings                   | Especially useful when positive class (churn) is rare                               |
| **Confusion Matrix**                      | Gives raw counts of TP, FP, FN, TN                                  | Useful for stakeholder communication                                                |
| **Lift at K / Top-K Recall**              | Tells you how many churners you caught in the top-scoring customers | Great for marketing prioritization (e.g., top 10% most likely churners)             |


âŒ __Accuracy:__ Misleading in imbalanced datasets. If 90% donâ€™t churn in learning data, a model predicting â€œno churnâ€ for everyone will still be 90% accurate â€” but useless.


##### ðŸ“– Churn Model Metrics Interpretation

| Metric               | What It Means                                       | Good Sign                          | Bad Sign                          | Special Notes for Churn                   |
| -------------------- | --------------------------------------------------- | ---------------------------------- | --------------------------------- | ----------------------------------------- |
| **Accuracy**         | % of all predictions that are correct               | >90%                               | Often misleading                  | Ignore this for churn (imbalanced data)   |
| **Precision**        | Of all predicted churners, how many really churned? | High = you donâ€™t annoy loyal users | Low = unnecessary retention costs | Important if retention actions are costly |
| **Recall**           | Of all real churners, how many did you catch?       | High = saves more customers        | Low = lost churners               | Most important in churn prevention        |
| **F1 Score**         | Balance of precision & recall                       | High = good tradeoff               | Low = biased model                | Use when both precision & recall matter   |
| **ROC AUC**          | How well does model rank churners vs. non-churners? | Close to 1.0                       | 0.5 = random guessing             | Good general performance metric           |
| **PR AUC**           | Area under Precision-Recall curve                   | High = effective for rare churn    | Low = poor signal on rare events  | Better than ROC AUC for rare churn        |
| **Confusion Matrix** | Raw count of TP, FP, FN, TN                         | -                                  | -                                 | Shows real-world mistakes                 |
| **Lift / Top-K**     | Not in this code, but useful for marketing          | -                                  | -                                 | Can be added later                        |
  
     
        
---  
  
### ðŸ”¥ How Rare Is "Rare"? Hold My Stake
| Churn Rate | Model                                         | Technique                     | Key Metric                |
| ---------- | --------------------------------------------- | ----------------------------- | ------------------------- |
| **< 20%**  | `LogisticRegression(class_weight='balanced')` | Balanced class weights        | ROC AUC + F1              |
| **< 10%**  | `SMOTE + LogisticRegression`                  | Oversampling + scaling        | PR AUC + F1               |
| **< 5%**   | `SMOTE + XGBClassifier(scale_pos_weight=...)` | Advanced model + class weight | PR AUC + threshold tuning |


